{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import sys,os\n",
    "sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0) \n",
    "\n",
    "import utils\n",
    "import csv\n",
    "\n",
    "import keras.activations as activations\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, TimeDistributed, BatchNormalization\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten, Lambda, Permute, RepeatVector\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def config():\n",
    "    c = dict()\n",
    "    # embedding params\n",
    "    c['emb'] = 'Glove'\n",
    "    c['embdim'] = 300\n",
    "    c['inp_e_dropout'] = 1/2\n",
    "\n",
    "    # training hyperparams\n",
    "    c['opt'] = 'adadelta'\n",
    "    c['batch_size'] = 160   \n",
    "    c['epochs'] = 16\n",
    "    c['patience'] = 3\n",
    "    \n",
    "    # sentences with word lengths below the 'pad' will be padded with 0.\n",
    "    c['pad'] = 60\n",
    "    \n",
    "    # rnn model\n",
    "    c['dropoutfix_inp'] = 0\n",
    "    c['dropoutfix_rec'] = 0           \n",
    "    c['dropout'] = 1/2     \n",
    "    c['l2reg'] = 1e-4\n",
    "                                              \n",
    "    c['rnnbidi'] = True                      \n",
    "    c['rnn'] = GRU                                                     \n",
    "    c['rnnbidi_mode'] = add\n",
    "    c['rnnact'] = 'tanh'\n",
    "    c['rnninit'] = 'glorot_uniform'                      \n",
    "    c['sdim'] = 1\n",
    "\n",
    "    # cnn model\n",
    "    c['cnn_dropout'] = 1/2     \n",
    "    c['pool_layer'] = MaxPooling1D\n",
    "    c['cnnact'] = 'relu'\n",
    "    c['cnninit'] = 'glorot_uniform'\n",
    "    c['cdim'] = 2\n",
    "    c['pact'] = 'tanh'\n",
    "\n",
    "    # projection layer\n",
    "    c['pdim'] = 1/2\n",
    "    c['p_layers'] = 1\n",
    "    c['p_dropout'] = 1/2\n",
    "    c['p_init'] = 'glorot_uniform'\n",
    "    \n",
    "    # attention model\n",
    "    c['adim'] = 1/2\n",
    "    c['cdim'] = 2\n",
    "    c['cfiltlen'] = 3\n",
    "\n",
    "    # mlp scoring function\n",
    "    c['Ddim'] = 1\n",
    "    \n",
    "    ps, h = utils.hash_params(c)\n",
    "\n",
    "    return c, ps, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = None\n",
    "emb = None\n",
    "vocab = None\n",
    "inp_tr = None\n",
    "inp_val = None\n",
    "inp_test = None\n",
    "y_val = None\n",
    "y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranknet(y_true, y_pred):\n",
    "    \"\"\" Bipartite ranking surrogate \"\"\"\n",
    "    return K.mean(K.log(1. + K.exp(-(y_true * y_pred - (1-y_true) * y_pred))), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The format of the dataset is as follows.\n",
    "\n",
    "question1, label, sentence1 \n",
    "question1, label, sentence2 \n",
    "         \n",
    "         ...\n",
    "                                         \n",
    "question2, label, sentence1 \n",
    "question2, label, sentence2 \n",
    "          \n",
    "         ...\n",
    "          \n",
    "questionN, label, sentenceM \n",
    "'''\n",
    "\n",
    "def load_data_from_file(dsfile):\n",
    "    #load a dataset in the csv format;\n",
    "    q = [] # a set of questions\n",
    "    sents = [] # a set of sentences\n",
    "    labels = [] # a set of labels\n",
    "\n",
    "    with open(dsfile) as f:\n",
    "        c = csv.DictReader(f)\n",
    "        for l in c:\n",
    "            label = int(l['label'])\n",
    "            labels.append(label)\n",
    "            try:\n",
    "                qtext = l['qtext'].decode('utf8')\n",
    "                stext = l['atext'].decode('utf8')\n",
    "            except AttributeError:  # python3 has no .decode()\n",
    "                qtext = l['qtext']\n",
    "                stext = l['atext']\n",
    "            \n",
    "            q.append(qtext.split(' '))\n",
    "            sents.append(stext.split(' '))\n",
    "            \n",
    "    return (q, sents, labels)\n",
    "    \n",
    "def make_model_inputs(qi, si, f01, f10, q, sents, y):\n",
    "    inp = {'qi': qi, 'si': si, 'f01':f01, 'f10':f10, 'q':q, 'sents':sents, 'y':y} \n",
    "    \n",
    "    return inp\n",
    "\n",
    "def load_set(fname, vocab=None, iseval=False):\n",
    "    q, sents, y = load_data_from_file(fname)\n",
    "    if not iseval:\n",
    "        vocab = utils.Vocabulary(q + sents) \n",
    "    \n",
    "    pad = conf['pad']\n",
    "    \n",
    "    qi = vocab.vectorize(q, pad=pad)  \n",
    "    si = vocab.vectorize(sents, pad=pad)        \n",
    "    f01, f10 = utils.sentence_flags(q, sents, pad)  \n",
    "    \n",
    "    inp = make_model_inputs(qi, si, f01, f10, q, sents, y)\n",
    "    if iseval:\n",
    "        return (inp, y)\n",
    "    else:\n",
    "        return (inp, y, vocab)        \n",
    "    \n",
    "def load_data(trainf, valf, testf):\n",
    "    global vocab, inp_tr, inp_val, inp_test, y_train, y_val, y_test\n",
    "    inp_tr, y_train, vocab = load_set(trainf, iseval=False)\n",
    "    inp_val, y_val = load_set(valf, vocab=vocab, iseval=True)\n",
    "    inp_test, y_test = load_set(testf, vocab=vocab, iseval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding():\n",
    "    '''\n",
    "    Declare all inputs (vectorized sentences and NLP flags)\n",
    "    and generate outputs representing vector sequences with dropout applied.  \n",
    "    Returns the vector dimensionality.       \n",
    "    '''\n",
    "    pad = conf['pad']\n",
    "    dropout = conf['inp_e_dropout']\n",
    "    \n",
    "    # story selection\n",
    "    input_qi = Input(name='qi', shape=(pad,), dtype='int32')                          \n",
    "    input_si = Input(name='si', shape=(pad,), dtype='int32')                 \n",
    "    input_f01 = Input(name='f01', shape=(pad, utils.flagsdim))\n",
    "    input_f10 = Input(name='f10', shape=(pad, utils.flagsdim))         \n",
    "\n",
    "    input_nodes = [input_qi, input_si, input_f01, input_f10]           \n",
    "        \n",
    "    N = emb.N + utils.flagsdim\n",
    "    shared_embedding = Embedding(name='emb', input_dim=vocab.size(), input_length=pad,\n",
    "                                output_dim=emb.N, mask_zero=True,\n",
    "                                weights=[vocab.embmatrix(emb)], trainable=True)\n",
    "    emb_qi = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_qi),\n",
    "        input_f01]))\n",
    "    emb_si = Dropout(dropout, noise_shape=(N,))(concatenate([shared_embedding(input_si),\n",
    "        input_f10]))\n",
    "\n",
    "    emb_outputs = [emb_qi, emb_si]\n",
    "    \n",
    "    return N, input_nodes, emb_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def projection_layer(inputs, input_size):\n",
    "    input0 = inputs[0]\n",
    "    input1 = inputs[1]\n",
    "    for p_i in range(conf['p_layers']):\n",
    "        shared_dense = Dense(name='pdeep%d'%(p_i), output_dim=int(input_size*conf['pdim']),\n",
    "                activation='linear', kernel_initializer=conf['p_init'], kernel_regularizer=l2(conf['l2reg']))\n",
    "        qi_proj = Activation(conf['pact'])(BatchNormalization()(shared_dense(input0)))\n",
    "        si_proj = Activation(conf['pact'])(BatchNormalization()(shared_dense(input1)))\n",
    "        input0 = qi_proj\n",
    "        input1 = si_proj\n",
    "        input_size = int(input_size * conf['pdim'])\n",
    "\n",
    "    dropout = conf['p_dropout']\n",
    "    qi_proj = Dropout(dropout, noise_shape=(input_size,))(qi_proj)\n",
    "    si_proj = Dropout(dropout, noise_shape=(input_size,))(si_proj)\n",
    "\n",
    "    return qi_proj, si_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_model(input_nodes, N, pfx=''):\n",
    "    shared_dense = Dense(int(N), activation='tanh', name='wproj'+pfx)\n",
    "    qi_wproj = TimeDistributed(shared_dense)(input_nodes[0])\n",
    "    si_wproj = TimeDistributed(shared_dense)(input_nodes[1])\n",
    "    \n",
    "    avg_layer = Lambda(name='bow'+pfx, function=lambda x: K.mean(x, axis=1), output_shape=lambda shape:(shape[0],) + shape[2:])\n",
    "    qi_avg = avg_layer(qi_wproj)\n",
    "    si_avg = avg_layer(si_wproj)\n",
    "\n",
    "    qi_avg, si_avg = projection_layer([qi_avg, si_avg], int(N))\n",
    "\n",
    "    return [qi_avg, si_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(input_nodes, N, pfx=''):\n",
    "    qi_cnn, si_cnn, nc = cnnsum_input(N, conf['pad'], dropout=conf['dropout'],\n",
    "                                l2reg=conf['l2reg'], cnninit=conf['cnninit'], cnnact=conf['cnnact'],\n",
    "                                inputs=input_nodes)\n",
    "    \n",
    "    qi_cnn, si_cnn = projection_layer([qi_cnn, si_cnn], nc)\n",
    "\n",
    "    return [qi_cnn, si_cnn]\n",
    "\n",
    "def cnnsum_input(N, pad, dropout=3/4, l2reg=1e-4, cnninit='glorot_uniform', cnnact='relu',\n",
    "        cdim={1: 1/2, 2: 1/2, 3: 1/2, 4: 1/2, 5: 1/2, 6: 1/2, 7: 1/2}, inputs=None, pfx=''):\n",
    "    qi_cnn_res_list = []\n",
    "    si_cnn_res_list = []\n",
    "    tot_len = 0\n",
    "    for fl, cd in cdim.items():\n",
    "        nb_filter = int(N*cd)\n",
    "        shared_conv = Convolution1D(name=pfx+'conv%d'%(fl), input_shape=(None, conf['pad'], N),\n",
    "                    kernel_size=fl, filters=nb_filter, activation='linear',\n",
    "                    kernel_regularizer=l2(l2reg), kernel_initializer=cnninit)\n",
    "        qi_cnn_one = Activation(cnnact)(BatchNormalization()(shared_conv(inputs[0])))\n",
    "        si_cnn_one = Activation(cnnact)(BatchNormalization()(shared_conv(inputs[1])))\n",
    "        \n",
    "        pool = MaxPooling1D(pool_size=int(conf['pad']-fl+1), name=pfx+'pool%d'%(fl))\n",
    "        qi_pool_one = pool(qi_cnn_one)\n",
    "        si_pool_one = pool(si_cnn_one)\n",
    "\n",
    "        flatten = Flatten(name=pfx+'flatten%d'%(fl))\n",
    "        qi_out_one = flatten(qi_pool_one)\n",
    "        si_out_one = flatten(si_pool_one)\n",
    "\n",
    "        qi_cnn_res_list.append(qi_out_one)\n",
    "        si_cnn_res_list.append(si_out_one)\n",
    "    \n",
    "        tot_len += nb_filter\n",
    "\n",
    "    qi_cnn = Dropout(dropout, noise_shape=(tot_len,))(concatenate(qi_cnn_res_list))\n",
    "    si_cnn = Dropout(dropout, noise_shape=(tot_len,))(concatenate(si_cnn_res_list))\n",
    "\n",
    "    return (qi_cnn, si_cnn, tot_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(input_nodes, N, pfx=''):\n",
    "    qi_rnn, si_rnn, nc = rnn_input(N, pfx=pfx, dropout=conf['dropout'], dropoutfix_inp=conf['dropoutfix_inp'], \n",
    "                            dropoutfix_rec=conf['dropoutfix_rec'], sdim=conf['sdim'], \n",
    "                            rnnbidi_mode=conf['rnnbidi_mode'], rnn=conf['rnn'], rnnact=conf['rnnact'], \n",
    "                            rnninit=conf['rnninit'], inputs=input_nodes)\n",
    "\n",
    "    qi_rnn, si_rnn = projection_layer([qi_rnn, si_rnn], nc)\n",
    "\n",
    "    return [qi_rnn, si_rnn]\n",
    "\n",
    "def rnn_input(N, dropout=3/4, dropoutfix_inp=0, dropoutfix_rec=0,           \n",
    "              sdim=2, rnn=GRU, rnnact='tanh', rnninit='glorot_uniform', rnnbidi_mode=add, \n",
    "              inputs=None, pfx=''):\n",
    "    if rnnbidi_mode == 'concat':\n",
    "        sdim /= 2\n",
    "    shared_rnn_f = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N), \n",
    "                       activation='linear', return_sequences=False, dropout=dropoutfix_inp,\n",
    "                       recurrent_dropout=dropoutfix_rec, name='rnnf'+pfx)\n",
    "    shared_rnn_b = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N),\n",
    "                       activation='linear', return_sequences=False, dropout=dropoutfix_inp,\n",
    "                       recurrent_dropout=dropoutfix_rec, go_backwards=True, name='rnnb'+pfx)\n",
    "    qi_rnn_f = Activation(rnnact)(BatchNormalization()(shared_rnn_f(inputs[0])))\n",
    "    si_rnn_f = Activation(rnnact)(BatchNormalization()(shared_rnn_f(inputs[1])))\n",
    "    \n",
    "    qi_rnn_b = Activation(rnnact)(BatchNormalization()(shared_rnn_b(inputs[0])))\n",
    "    si_rnn_b = Activation(rnnact)(BatchNormalization()(shared_rnn_b(inputs[1])))\n",
    "    \n",
    "    qi_rnn = Dropout(dropout, noise_shape=(int(N*sdim),))(rnnbidi_mode([qi_rnn_f, qi_rnn_b]))\n",
    "    si_rnn = Dropout(dropout, noise_shape=(int(N*sdim),))(rnnbidi_mode([si_rnn_f, si_rnn_b]))\n",
    "    \n",
    "    return (qi_rnn, si_rnn, int(N*sdim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_ptscorer(inputs, Ddim, N, l2reg, pfx='out', oact='sigmoid', extra_inp=[]):\n",
    "    \"\"\" Element-wise features from the pair fed to an MLP. \"\"\"\n",
    "\n",
    "    sum_vec = add(inputs)\n",
    "    mul_vec = multiply(inputs)\n",
    "\n",
    "    mlp_input = concatenate([sum_vec, mul_vec])\n",
    "\n",
    "    # Ddim may be either 0 (no hidden layer), scalar (single hidden layer) or\n",
    "    # list (multiple hidden layers)\n",
    "    if Ddim == 0:\n",
    "        Ddim = []\n",
    "    elif not isinstance(Ddim, list):\n",
    "        Ddim = [Ddim]\n",
    "    if Ddim:\n",
    "        for i, D in enumerate(Ddim):\n",
    "            shared_dense = Dense(int(N*D), kernel_regularizer=l2(l2reg), \n",
    "                                 activation='tanh', name=pfx+'hdn%d'%(i))\n",
    "            mlp_input = shared_dense(mlp_input)\n",
    "\n",
    "    shared_dense = Dense(1, kernel_regularizer=l2(l2reg), activation=oact, name=pfx+'mlp')\n",
    "    mlp_out = shared_dense(mlp_input)\n",
    "    \n",
    "    return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # input embedding         \n",
    "    N, input_nodes_emb, output_nodes_emb = embedding()\n",
    "    \n",
    "    # answer sentence selection\n",
    "    ptscorer_inputs1 = cnn_model(output_nodes_emb, N, pfx='S')\n",
    "\n",
    "    scoreS1 = mlp_ptscorer(ptscorer_inputs1, conf['Ddim'], N,  \n",
    "            conf['l2reg'], pfx='outS', oact='sigmoid')                \n",
    "\n",
    "    output_nodes = scoreS1\n",
    "\n",
    "    model = Model(inputs=input_nodes_emb, outputs=output_nodes)\n",
    "    \n",
    "    model.compile(loss=ranknet, optimizer=conf['opt'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(runid):\n",
    "    print('Model')\n",
    "    model = build_model()\n",
    "    print(model.summary())\n",
    "    \n",
    "    print('Training')\n",
    "    fit_model(model, weightsf='weights-'+runid+'-bestval.h5')\n",
    "    model.save_weights('weights-'+runid+'-final.h5', overwrite=True)\n",
    "    model.load_weights('weights-'+runid+'-bestval.h5')\n",
    "\n",
    "    print('Predict&Eval (best val epoch)')\n",
    "    res = eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, **kwargs):\n",
    "    epochs = conf['epochs']\n",
    "    callbacks = fit_callbacks(kwargs.pop('weightsf'))\n",
    "    \n",
    "    return model.fit(inp_tr, y=y_train, validation_data=[inp_val, y_val], \n",
    "                     callbacks = callbacks, epochs=epochs)\n",
    "\n",
    "def fit_callbacks(weightsf):                                  \n",
    "    return [utils.AnsSelCB(inp_val['q'], inp_val['sents'], y_val, inp_val),\n",
    "            ModelCheckpoint(weightsf, save_best_only=True, monitor='mrr', mode='max'),\n",
    "            EarlyStopping(monitor='mrr', mode='max', patience=conf['patience'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    res = []\n",
    "    for inp in [inp_val, inp_test]:\n",
    "        if inp is None:\n",
    "            res.append(None)\n",
    "            continue\n",
    "\n",
    "        pred = model.predict(inp)\n",
    "        res.append(utils.eval_QA(pred, inp['q'], inp['y'], MAP=False))\n",
    "    return tuple(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainf = 'data/train-all.csv' \n",
    "    valf = 'data/dev.csv'\n",
    "    testf = 'data/test.csv'\n",
    "    params = []\n",
    "    \n",
    "    conf, ps, h = config()\n",
    "\n",
    "    if conf['emb'] == 'Glove':\n",
    "        print('GloVe')\n",
    "        emb = utils.GloVe(N=conf['embdim'])\n",
    "\n",
    "    print('Dataset')\n",
    "    load_data(trainf,valf,testf)\n",
    "    runid = 'Model-%x' % (h)\n",
    "    print('RunID: %s  (%s)' % (runid, ps))\n",
    "    train_and_eval(runid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
